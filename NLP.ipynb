{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter some textThere are three reasons why I prefer jogging to other sports. One reason is that jogging is a cheap sport. I can practise it anywhere at any time with no need for a ball or any other equipment. Another reason why I prefer jogging is that it is friendly to my heart. I don’t have to exhaust myself or do excessive efforts while jogging. Finally, I prefer this sport because it is safe. It isn’t as risky as other sports like gymnastics, racing or horseback riding. For all these reasons, I consider jogging the best sport of all.\n"
     ]
    }
   ],
   "source": [
    "user_text = input(\"Enter some text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There are three reasons why I prefer jogging to other sports.',\n",
       " 'One reason is that jogging is a cheap sport.',\n",
       " 'I can practise it anywhere at any time with no need for a ball or any other equipment.',\n",
       " 'Another reason why I prefer jogging is that it is friendly to my heart.',\n",
       " 'I don’t have to exhaust myself or do excessive efforts while jogging.',\n",
       " 'Finally, I prefer this sport because it is safe.',\n",
       " 'It isn’t as risky as other sports like gymnastics, racing or horseback riding.',\n",
       " 'For all these reasons, I consider jogging the best sport of all.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = sent_tokenize(user_text)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'three',\n",
       " 'reasons',\n",
       " 'why',\n",
       " 'I',\n",
       " 'prefer',\n",
       " 'jogging',\n",
       " 'to',\n",
       " 'other',\n",
       " 'sports',\n",
       " '.',\n",
       " 'One',\n",
       " 'reason',\n",
       " 'is',\n",
       " 'that',\n",
       " 'jogging',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'sport',\n",
       " '.',\n",
       " 'I',\n",
       " 'can',\n",
       " 'practise',\n",
       " 'it',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'any',\n",
       " 'time',\n",
       " 'with',\n",
       " 'no',\n",
       " 'need',\n",
       " 'for',\n",
       " 'a',\n",
       " 'ball',\n",
       " 'or',\n",
       " 'any',\n",
       " 'other',\n",
       " 'equipment',\n",
       " '.',\n",
       " 'Another',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'I',\n",
       " 'prefer',\n",
       " 'jogging',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'friendly',\n",
       " 'to',\n",
       " 'my',\n",
       " 'heart',\n",
       " '.',\n",
       " 'I',\n",
       " 'don',\n",
       " '’',\n",
       " 't',\n",
       " 'have',\n",
       " 'to',\n",
       " 'exhaust',\n",
       " 'myself',\n",
       " 'or',\n",
       " 'do',\n",
       " 'excessive',\n",
       " 'efforts',\n",
       " 'while',\n",
       " 'jogging',\n",
       " '.',\n",
       " 'Finally',\n",
       " ',',\n",
       " 'I',\n",
       " 'prefer',\n",
       " 'this',\n",
       " 'sport',\n",
       " 'because',\n",
       " 'it',\n",
       " 'is',\n",
       " 'safe',\n",
       " '.',\n",
       " 'It',\n",
       " 'isn',\n",
       " '’',\n",
       " 't',\n",
       " 'as',\n",
       " 'risky',\n",
       " 'as',\n",
       " 'other',\n",
       " 'sports',\n",
       " 'like',\n",
       " 'gymnastics',\n",
       " ',',\n",
       " 'racing',\n",
       " 'or',\n",
       " 'horseback',\n",
       " 'riding',\n",
       " '.',\n",
       " 'For',\n",
       " 'all',\n",
       " 'these',\n",
       " 'reasons',\n",
       " ',',\n",
       " 'I',\n",
       " 'consider',\n",
       " 'jogging',\n",
       " 'the',\n",
       " 'best',\n",
       " 'sport',\n",
       " 'of',\n",
       " 'all',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words = word_tokenize(user_text)\n",
    "tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 8, 'I': 6, 'jogging': 5, 'is': 5, 'prefer': 3, 'to': 3, 'other': 3, 'sport': 3, 'it': 3, 'or': 3, ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_of_words = FreqDist(tokenized_words)\n",
    "count_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 56)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = []\n",
    "for word in tokenized_words:\n",
    "    if word == '.':\n",
    "        continue\n",
    "    if not word in stopwords_list:\n",
    "        clean_text.append(word)\n",
    "        \n",
    "len(tokenized_words), len(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'three',\n",
       " 'reasons',\n",
       " 'I',\n",
       " 'prefer',\n",
       " 'jogging',\n",
       " 'sports',\n",
       " 'One',\n",
       " 'reason',\n",
       " 'jogging',\n",
       " 'cheap',\n",
       " 'sport',\n",
       " 'I',\n",
       " 'practise',\n",
       " 'anywhere',\n",
       " 'time',\n",
       " 'need',\n",
       " 'ball',\n",
       " 'equipment',\n",
       " 'Another',\n",
       " 'reason',\n",
       " 'I',\n",
       " 'prefer',\n",
       " 'jogging',\n",
       " 'friendly',\n",
       " 'heart',\n",
       " 'I',\n",
       " '’',\n",
       " 'exhaust',\n",
       " 'excessive',\n",
       " 'efforts',\n",
       " 'jogging',\n",
       " 'Finally',\n",
       " ',',\n",
       " 'I',\n",
       " 'prefer',\n",
       " 'sport',\n",
       " 'safe',\n",
       " 'It',\n",
       " '’',\n",
       " 'risky',\n",
       " 'sports',\n",
       " 'like',\n",
       " 'gymnastics',\n",
       " ',',\n",
       " 'racing',\n",
       " 'horseback',\n",
       " 'riding',\n",
       " 'For',\n",
       " 'reasons',\n",
       " ',',\n",
       " 'I',\n",
       " 'consider',\n",
       " 'jogging',\n",
       " 'best',\n",
       " 'sport']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jog'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jogging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fli'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('flying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'featur'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'race'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('racing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'put'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('Put')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'industri'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('industrialization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There -> there\n",
      "are -> are\n",
      "three -> three\n",
      "reasons -> reason\n",
      "why -> whi\n",
      "I -> I\n",
      "prefer -> prefer\n",
      "jogging -> jog\n",
      "to -> to\n",
      "other -> other\n",
      "sports -> sport\n",
      ". -> .\n",
      "One -> one\n",
      "reason -> reason\n",
      "is -> is\n",
      "that -> that\n",
      "jogging -> jog\n",
      "is -> is\n",
      "a -> a\n",
      "cheap -> cheap\n",
      "sport -> sport\n",
      ". -> .\n",
      "I -> I\n",
      "can -> can\n",
      "practise -> practis\n",
      "it -> it\n",
      "anywhere -> anywher\n",
      "at -> at\n",
      "any -> ani\n",
      "time -> time\n",
      "with -> with\n",
      "no -> no\n",
      "need -> need\n",
      "for -> for\n",
      "a -> a\n",
      "ball -> ball\n",
      "or -> or\n",
      "any -> ani\n",
      "other -> other\n",
      "equipment -> equip\n",
      ". -> .\n",
      "Another -> anoth\n",
      "reason -> reason\n",
      "why -> whi\n",
      "I -> I\n",
      "prefer -> prefer\n",
      "jogging -> jog\n",
      "is -> is\n",
      "that -> that\n",
      "it -> it\n",
      "is -> is\n",
      "friendly -> friendli\n",
      "to -> to\n",
      "my -> my\n",
      "heart -> heart\n",
      ". -> .\n",
      "I -> I\n",
      "don -> don\n",
      "’ -> ’\n",
      "t -> t\n",
      "have -> have\n",
      "to -> to\n",
      "exhaust -> exhaust\n",
      "myself -> myself\n",
      "or -> or\n",
      "do -> do\n",
      "excessive -> excess\n",
      "efforts -> effort\n",
      "while -> while\n",
      "jogging -> jog\n",
      ". -> .\n",
      "Finally -> final\n",
      ", -> ,\n",
      "I -> I\n",
      "prefer -> prefer\n",
      "this -> thi\n",
      "sport -> sport\n",
      "because -> becaus\n",
      "it -> it\n",
      "is -> is\n",
      "safe -> safe\n",
      ". -> .\n",
      "It -> It\n",
      "isn -> isn\n",
      "’ -> ’\n",
      "t -> t\n",
      "as -> as\n",
      "risky -> riski\n",
      "as -> as\n",
      "other -> other\n",
      "sports -> sport\n",
      "like -> like\n",
      "gymnastics -> gymnast\n",
      ", -> ,\n",
      "racing -> race\n",
      "or -> or\n",
      "horseback -> horseback\n",
      "riding -> ride\n",
      ". -> .\n",
      "For -> for\n",
      "all -> all\n",
      "these -> these\n",
      "reasons -> reason\n",
      ", -> ,\n",
      "I -> I\n",
      "consider -> consid\n",
      "jogging -> jog\n",
      "the -> the\n",
      "best -> best\n",
      "sport -> sport\n",
      "of -> of\n",
      "all -> all\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "for word in tokenized_words:\n",
    "    print(word,\"->\",ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flying'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize('flying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feature'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('three', 'CD'),\n",
       " ('reasons', 'NNS'),\n",
       " ('why', 'WRB'),\n",
       " ('I', 'PRP'),\n",
       " ('prefer', 'VBP'),\n",
       " ('jogging', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('other', 'JJ'),\n",
       " ('sports', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('One', 'CD'),\n",
       " ('reason', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('jogging', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('cheap', 'JJ'),\n",
       " ('sport', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('practise', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('anywhere', 'RB'),\n",
       " ('at', 'IN'),\n",
       " ('any', 'DT'),\n",
       " ('time', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('no', 'DT'),\n",
       " ('need', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('ball', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('any', 'DT'),\n",
       " ('other', 'JJ'),\n",
       " ('equipment', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Another', 'DT'),\n",
       " ('reason', 'NN'),\n",
       " ('why', 'WRB'),\n",
       " ('I', 'PRP'),\n",
       " ('prefer', 'VBP'),\n",
       " ('jogging', 'VBG'),\n",
       " ('is', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('friendly', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('heart', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('don', 'VBP'),\n",
       " ('’', 'JJ'),\n",
       " ('t', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('exhaust', 'VB'),\n",
       " ('myself', 'PRP'),\n",
       " ('or', 'CC'),\n",
       " ('do', 'VB'),\n",
       " ('excessive', 'JJ'),\n",
       " ('efforts', 'NNS'),\n",
       " ('while', 'IN'),\n",
       " ('jogging', 'VBG'),\n",
       " ('.', '.'),\n",
       " ('Finally', 'RB'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('prefer', 'VBP'),\n",
       " ('this', 'DT'),\n",
       " ('sport', 'NN'),\n",
       " ('because', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('safe', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('isn', 'JJ'),\n",
       " ('’', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('as', 'RB'),\n",
       " ('risky', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('other', 'JJ'),\n",
       " ('sports', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('gymnastics', 'NNS'),\n",
       " (',', ','),\n",
       " ('racing', 'VBG'),\n",
       " ('or', 'CC'),\n",
       " ('horseback', 'NN'),\n",
       " ('riding', 'NN'),\n",
       " ('.', '.'),\n",
       " ('For', 'IN'),\n",
       " ('all', 'PDT'),\n",
       " ('these', 'DT'),\n",
       " ('reasons', 'NNS'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('consider', 'VBP'),\n",
       " ('jogging', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('best', 'JJS'),\n",
       " ('sport', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_messages = []\n",
    "\n",
    "for i in range(tweet_df.shape[0]):\n",
    "    clean_text = ''\n",
    "    current_text = word_tokenize(tweet_df['text'].values[i])\n",
    "    for word in current_text:\n",
    "        if word == '@' or word == 'VirginAmerica' or word == 'United' or word == 'SouthwestAir' or word == 'USAirways' or word == 'americanair' or word == 'AmericanAir' or word == 'jetblue' or word == 'delta' or word == 'Delta':\n",
    "            continue\n",
    "        if not word in stopwords_list:\n",
    "            clean_text = clean_text + ' ' + word\n",
    "        \n",
    "    clean_messages.append(clean_text)\n",
    "\n",
    "len(clean_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['clean_text'] = clean_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>What dhepburn said .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>plus 've added commercials experience ... tac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>I n't today ... Must mean I need take another...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>'s really aggressive blast obnoxious `` enter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>'s really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>thank got different flight Chicago .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>leaving 20 minutes Late Flight . No warnings ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>Please bring American Airlines # BlackBerry10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>money , change flight , n't answer phones ! A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>8 ppl need 2 know many seats next flight . Pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0                    @VirginAmerica What @dhepburn said.   \n",
       "1      @VirginAmerica plus you've added commercials t...   \n",
       "2      @VirginAmerica I didn't today... Must mean I n...   \n",
       "3      @VirginAmerica it's really aggressive to blast...   \n",
       "4      @VirginAmerica and it's a really big bad thing...   \n",
       "...                                                  ...   \n",
       "14635  @AmericanAir thank you we got on a different f...   \n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...   \n",
       "14637  @AmericanAir Please bring American Airlines to...   \n",
       "14638  @AmericanAir you have my money, you change my ...   \n",
       "14639  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
       "\n",
       "                                              clean_text  \n",
       "0                                   What dhepburn said .  \n",
       "1       plus 've added commercials experience ... tac...  \n",
       "2       I n't today ... Must mean I need take another...  \n",
       "3       's really aggressive blast obnoxious `` enter...  \n",
       "4                                's really big bad thing  \n",
       "...                                                  ...  \n",
       "14635               thank got different flight Chicago .  \n",
       "14636   leaving 20 minutes Late Flight . No warnings ...  \n",
       "14637      Please bring American Airlines # BlackBerry10  \n",
       "14638   money , change flight , n't answer phones ! A...  \n",
       "14639   8 ppl need 2 know many seats next flight . Pl...  \n",
       "\n",
       "[14640 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df[['text','clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(tweet_df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14640x15020 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 146482 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>What dhepburn said .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>plus 've added commercials experience ... tac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>I n't today ... Must mean I need take another...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>'s really aggressive blast obnoxious `` enter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>'s really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>569587686496825344</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 12:01:01 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thank got different flight Chicago .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355008</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:46 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leaving 20 minutes Late Flight . No warnings ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398336</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:15 -0800</td>\n",
       "      <td>Nigeria,lagos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please bring American Airlines # BlackBerry10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634433</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:02 -0800</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>money , change flight , n't answer phones ! A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866689</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:58:51 -0800</td>\n",
       "      <td>dallas, TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8 ppl need 2 know many seats next flight . Pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0      570306133677760513           neutral                        1.0000   \n",
       "1      570301130888122368          positive                        0.3486   \n",
       "2      570301083672813571           neutral                        0.6837   \n",
       "3      570301031407624196          negative                        1.0000   \n",
       "4      570300817074462722          negative                        1.0000   \n",
       "...                   ...               ...                           ...   \n",
       "14635  569587686496825344          positive                        0.3487   \n",
       "14636  569587371693355008          negative                        1.0000   \n",
       "14637  569587242672398336           neutral                        1.0000   \n",
       "14638  569587188687634433          negative                        1.0000   \n",
       "14639  569587140490866689           neutral                        0.6771   \n",
       "\n",
       "               negativereason  negativereason_confidence         airline  \\\n",
       "0                         NaN                        NaN  Virgin America   \n",
       "1                         NaN                     0.0000  Virgin America   \n",
       "2                         NaN                        NaN  Virgin America   \n",
       "3                  Bad Flight                     0.7033  Virgin America   \n",
       "4                  Can't Tell                     1.0000  Virgin America   \n",
       "...                       ...                        ...             ...   \n",
       "14635                     NaN                     0.0000        American   \n",
       "14636  Customer Service Issue                     1.0000        American   \n",
       "14637                     NaN                        NaN        American   \n",
       "14638  Customer Service Issue                     0.6659        American   \n",
       "14639                     NaN                     0.0000        American   \n",
       "\n",
       "      airline_sentiment_gold             name negativereason_gold  \\\n",
       "0                        NaN          cairdin                 NaN   \n",
       "1                        NaN         jnardino                 NaN   \n",
       "2                        NaN       yvonnalynn                 NaN   \n",
       "3                        NaN         jnardino                 NaN   \n",
       "4                        NaN         jnardino                 NaN   \n",
       "...                      ...              ...                 ...   \n",
       "14635                    NaN  KristenReenders                 NaN   \n",
       "14636                    NaN         itsropes                 NaN   \n",
       "14637                    NaN         sanyabun                 NaN   \n",
       "14638                    NaN       SraJackson                 NaN   \n",
       "14639                    NaN        daviddtwu                 NaN   \n",
       "\n",
       "       retweet_count                                               text  \\\n",
       "0                  0                @VirginAmerica What @dhepburn said.   \n",
       "1                  0  @VirginAmerica plus you've added commercials t...   \n",
       "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
       "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
       "...              ...                                                ...   \n",
       "14635              0  @AmericanAir thank you we got on a different f...   \n",
       "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
       "14637              0  @AmericanAir Please bring American Airlines to...   \n",
       "14638              0  @AmericanAir you have my money, you change my ...   \n",
       "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
       "\n",
       "      tweet_coord              tweet_created tweet_location  \\\n",
       "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "...           ...                        ...            ...   \n",
       "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
       "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
       "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
       "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
       "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
       "\n",
       "                    user_timezone  \\\n",
       "0      Eastern Time (US & Canada)   \n",
       "1      Pacific Time (US & Canada)   \n",
       "2      Central Time (US & Canada)   \n",
       "3      Pacific Time (US & Canada)   \n",
       "4      Pacific Time (US & Canada)   \n",
       "...                           ...   \n",
       "14635                         NaN   \n",
       "14636                         NaN   \n",
       "14637                         NaN   \n",
       "14638  Eastern Time (US & Canada)   \n",
       "14639                         NaN   \n",
       "\n",
       "                                              clean_text  \n",
       "0                                   What dhepburn said .  \n",
       "1       plus 've added commercials experience ... tac...  \n",
       "2       I n't today ... Must mean I need take another...  \n",
       "3       's really aggressive blast obnoxious `` enter...  \n",
       "4                                's really big bad thing  \n",
       "...                                                  ...  \n",
       "14635               thank got different flight Chicago .  \n",
       "14636   leaving 20 minutes Late Flight . No warnings ...  \n",
       "14637      Please bring American Airlines # BlackBerry10  \n",
       "14638   money , change flight , n't answer phones ! A...  \n",
       "14639   8 ppl need 2 know many seats next flight . Pl...  \n",
       "\n",
       "[14640 rows x 16 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, tweet_df['airline_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7565573770491804"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14526)\t1\n",
      "  (0, 4797)\t1\n",
      "  (0, 11627)\t1\n",
      "  (1, 10419)\t1\n",
      "  (1, 14170)\t1\n",
      "  (1, 1964)\t1\n",
      "  (1, 4025)\t1\n",
      "  (1, 5657)\t1\n",
      "  (1, 12905)\t1\n",
      "  (2, 13309)\t1\n",
      "  (2, 9217)\t1\n",
      "  (2, 8820)\t1\n",
      "  (2, 9358)\t1\n",
      "  (2, 12921)\t1\n",
      "  (2, 2329)\t1\n",
      "  (2, 13488)\t1\n",
      "  (3, 11001)\t1\n",
      "  (3, 2052)\t1\n",
      "  (3, 3066)\t1\n",
      "  (3, 9709)\t1\n",
      "  (3, 5447)\t1\n",
      "  (3, 6724)\t1\n",
      "  (3, 5732)\t1\n",
      "  (3, 2261)\t1\n",
      "  (3, 8375)\t1\n",
      "  :\t:\n",
      "  (14637, 2102)\t1\n",
      "  (14637, 3259)\t1\n",
      "  (14637, 2234)\t1\n",
      "  (14637, 3048)\t1\n",
      "  (14638, 5996)\t1\n",
      "  (14638, 3674)\t1\n",
      "  (14638, 2353)\t1\n",
      "  (14638, 2334)\t1\n",
      "  (14638, 9096)\t1\n",
      "  (14638, 8657)\t1\n",
      "  (14638, 10284)\t1\n",
      "  (14638, 4028)\t1\n",
      "  (14638, 12718)\t1\n",
      "  (14639, 9358)\t1\n",
      "  (14639, 5996)\t2\n",
      "  (14639, 11814)\t1\n",
      "  (14639, 8059)\t1\n",
      "  (14639, 9430)\t2\n",
      "  (14639, 10420)\t1\n",
      "  (14639, 14026)\t1\n",
      "  (14639, 10203)\t1\n",
      "  (14639, 8704)\t1\n",
      "  (14639, 12492)\t1\n",
      "  (14639, 10514)\t1\n",
      "  (14639, 10798)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
